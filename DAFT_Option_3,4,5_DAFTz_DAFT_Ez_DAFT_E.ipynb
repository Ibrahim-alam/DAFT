{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ibrahim-alam/DAFT/blob/main/DAFT_Option_3%2C4%2C5_DAFTz_DAFT_Ez_DAFT_E.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNR5ko_jN9hY"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho1TjaIZOFw3",
        "outputId": "8eaa6a33-c92e-4e3b-dbf2-f5dc15c53428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers huggingface_hub\n",
        "!apt-get install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "rX_A1j_YOpsn",
        "outputId": "f169ba14-633f-4747-967f-c45a9341d1a6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "HfHubHTTPError",
          "evalue": "500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/stanfordnlp/imdb (Request ID: Root=1-66a52ec0-6c7328a0055891417888754f;66cb9d2b-0841-41e4-863e-b0abc2e85e4e)\n\nInternal Error - We're working hard to fix this as soon as possible!",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-25f7846ecf97>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdataset_name_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_load_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdataset2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name_load\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;31m# number of classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtest_dataset2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2593\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2594\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   2595\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\u001b[0m\n\u001b[1;32m   2264\u001b[0m         \u001b[0mdownload_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdownload_config\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2265\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2266\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   2267\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2268\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1912\u001b[0m                         \u001b[0;34mf\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m                     ) from None\n\u001b[0;32m-> 1914\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1915\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m         raise FileNotFoundError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\u001b[0m\n\u001b[1;32m   1832\u001b[0m             \u001b[0mhf_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHF_ENDPOINT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1834\u001b[0;31m                 dataset_info = hf_api.dataset_info(\n\u001b[0m\u001b[1;32m   1835\u001b[0m                     \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1836\u001b[0m                     \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mdataset_info\u001b[0;34m(self, repo_id, revision, timeout, files_metadata, token)\u001b[0m\n\u001b[1;32m   2362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2364\u001b[0;31m         \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2365\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDatasetInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/stanfordnlp/imdb (Request ID: Root=1-66a52ec0-6c7328a0055891417888754f;66cb9d2b-0841-41e4-863e-b0abc2e85e4e)\n\nInternal Error - We're working hard to fix this as soon as possible!"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "\n",
        "dataset_names = ['yelp_polarity', 'amazon_polarity', 'sst2', 'tweet_sentiment', 'cornel_sentiment',  'imdb']\n",
        "dataset_load_names = ['yelp_polarity', 'amazon_polarity', 'sst2', 'Ibrahim-Alam/tweet_sentiment_pos_neg','Ibrahim-Alam/cornel_sentiment', 'imdb']\n",
        "\n",
        "idx = 5\n",
        "dataset_name_load = dataset_load_names[idx]\n",
        "dataset_name = dataset_names[idx]\n",
        "dataset2 = load_dataset(dataset_name_load)\n",
        "K = 2 # number of classes\n",
        "test_dataset2 = dataset2[\"test\"].shuffle(seed=42)\n",
        "feature_label = 'label' #'label' for sst2, twitter and imdb, 'Label' for cornell,\n",
        "\n",
        "if dataset_name == 'tweet':\n",
        "  test_dataset2 =  dataset2[\"train\"].shuffle(seed=42).select([i for i in list(range(7000, 14000))]) #dataset2[\"validation\"].shuffle(seed=1234)\n",
        "  feature_text = 'sentence' #'sentence' for sst2, 'text' for IMDB and Twitter, 'Text' for cornell,\n",
        "elif dataset_name == 'amazon_polarity':\n",
        "  feature_text = 'content' #using content, could have used title only or both together.\n",
        "else:\n",
        "  feature_text = 'text'\n",
        "\n",
        "test_data_len = 5000\n",
        "if len(test_dataset2)>test_data_len:\n",
        "  test_dataset2 = dataset2[\"test\"].shuffle(seed=42).shuffle(seed=42).select([i for i in list(range(test_data_len))])\n",
        "\n",
        "y_test = test_dataset2['label']\n",
        "y_test_np = np.array(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYzoaP_l2pt1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from transformers.pipelines.pt_utils import KeyDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3gaopz_xIYP"
      },
      "outputs": [],
      "source": [
        "model_names = ['Ibrahim-Alam/finetuning-roberta-base-on-yelp_polarity_7000_samples', #0\n",
        "               'Ibrahim-Alam/finetuning-roberta-base-on-amazon_polarity_7000_samples', #1\n",
        "               'Ibrahim-Alam/finetuning-roberta-base-on-sst2_7000_samples',#2\n",
        "               'Ibrahim-Alam/finetuning_roberta_on_Tweet_Sentiment_pos_neg_7000_samples',#3\n",
        "               'Ibrahim-Alam/finetuning_roberta_on_cornel_sentiment_7000_samples',#4\n",
        "               'Ibrahim-Alam/finetuning-roberta-base-on-imdb_7000_samples',#5\n",
        "               #####\n",
        "               'Ibrahim-Alam/finetuning-bert-base-uncased-on-yelp_polarity_7000_samples', # 6\n",
        "               'Ibrahim-Alam/finetuning-bert-base-uncased-on-amazon_polarity_7000_samples', # 7\n",
        "               'Ibrahim-Alam/finetuning-bert-base-uncased-on-sst2_7000_samples', # 8\n",
        "               'Ibrahim-Alam/finetuning_bert_uncased_on_Tweet_Sentiment_pos_neg_7000_samples', # 9\n",
        "               'Ibrahim-Alam/finetuning_bert_uncased_on_cornel_sentiment_7000_samples', # 10\n",
        "               'Ibrahim-Alam/finetuning-bert-base-uncased-on-imdb_7000_samples', # 11\n",
        "               #####\n",
        "               'Ibrahim-Alam/finetuning-xlnet-base-cased-on-yelp_polarity_7000_samples', # 12\n",
        "               'Ibrahim-Alam/finetuning-xlnet-base-cased-on-amazon_polarity_7000_samples', # 13\n",
        "               'Ibrahim-Alam/finetuning-xlnet-base-cased-on-sst2_7000_samples', # 14\n",
        "               'Ibrahim-Alam/finetuning_xlnet_on_Tweet_Sentiment_pos_neg_7000_samples', # 15\n",
        "               'Ibrahim-Alam/finetuning_xlnet_on_cornel_sentiment_7000_samples', # 16\n",
        "               'Ibrahim-Alam/finetuning-xlnet-base-cased-on-imdb_7000_samples', # 17\n",
        "               ]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Option 3 of DAFT - *DAFT^z***\n",
        "## DAFT^z : Using DAFT models directly on the target domain and check the performance\n"
      ],
      "metadata": {
        "id": "_A-5AmdJnwoK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aiRaXUO5Q9T"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "time1 = time.time()\n",
        "unwanted_index = [5, 11, 17]\n",
        "all_indx = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
        "indx =  [v for i,v in enumerate(all_indx) if i not in unwanted_index]\n",
        "\n",
        "y_pred = np.zeros((len(test_dataset2),len(indx)))\n",
        "y_prob = np.zeros((len(test_dataset2),len(indx)))\n",
        "Accr = np.zeros((len(indx),1))\n",
        "for i in range(len(indx)):\n",
        "  repo_name = model_names[indx[i]]\n",
        "  pipe = pipeline(model=repo_name, device=0, token = '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')\n",
        "  y_pred1 = []\n",
        "  y_probability1 = []\n",
        "  p = 0\n",
        "  for out in pipe(KeyDataset(test_dataset2, feature_text), batch_size=4, truncation=\"only_first\"):\n",
        "    p += 1\n",
        "    if out['label'] == 'LABEL_1':\n",
        "      y_pred1.append(1)\n",
        "    elif out['label'] == 'LABEL_0':\n",
        "      y_pred1.append(0)\n",
        "    y_probability1.append(out['score'])\n",
        "\n",
        "  y_pred1_np = np.array(y_pred1)\n",
        "  Accuracy1  = np.sum(y_pred1_np==y_test_np)/len(y_pred1)\n",
        "  print(Accuracy1)\n",
        "  y_pred[:,i] = y_pred1\n",
        "  y_prob[:,i] = y_probability1\n",
        "  Accr[i] = Accuracy1\n",
        "  torch.cuda.empty_cache()\n",
        "  del model\n",
        "time2 = time.time()\n",
        "print(time2-time1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2o0KJGMZqLdi"
      },
      "outputs": [],
      "source": [
        "Out_probs = np.zeros((len(test_dataset2),len(indx)))\n",
        "for i in range(len(test_dataset2)):\n",
        "  for j in range(len(indx)):\n",
        "    if y_pred[i,j] == 1:\n",
        "      Out_probs[i,j] = 0.5 + (y_prob[i,j])/2\n",
        "    elif y_pred[i,j] == 0:\n",
        "      Out_probs[i,j] = 0.5 - (y_prob[i,j])/2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Option 4 of DAFT - *DAFT-E^z***\n",
        "## DAFT-E^z : Using the output from all DAFT models and perform an Ensemble on the output to get DAFT-E^z output"
      ],
      "metadata": {
        "id": "LJ3lym0VoOnb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oL-42qPAhqVF"
      },
      "outputs": [],
      "source": [
        "Out_prob = np.sum(Out_probs, axis=1)/len(indx)\n",
        "Y_Pred_prob = np.zeros((len(Out_prob)))\n",
        "Out_max_vote = np.sum(y_pred, axis=1)/len(indx)\n",
        "Y_Pred_max = np.zeros((len(Out_prob)))\n",
        "\n",
        "for i in range(len(Out_prob)):\n",
        "  if Out_prob[i]>=0.5:\n",
        "    Y_Pred_prob[i] = 1\n",
        "  elif Out_prob[i]<0.5:\n",
        "    Y_Pred_prob[i] = 0\n",
        "\n",
        "  if Out_max_vote[i] >= 0.5:\n",
        "    Y_Pred_max[i] = 1\n",
        "  elif Out_max_vote[i] < 0.5:\n",
        "    Y_Pred_max[i] = 0\n",
        "\n",
        "\n",
        "Acc_prob = np.sum(Y_Pred_prob == y_test_np)/len(y_test_np)\n",
        "Acc_max_voting = np.sum(Y_Pred_max == y_test_np)/len(y_test_np)\n",
        "print('Accuracy: '+ str(Acc_prob))\n",
        "print('Accuracy: '+ str(Acc_max_voting))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZzihO4im8-4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "!mkdir Results\n",
        "\n",
        "\n",
        "Single_and_Ensemble = pd.DataFrame(columns = ['Name','Accuracy'])\n",
        "count = 0\n",
        "for i in range(len(model_names)):\n",
        "  if i in indx:\n",
        "    temp1 = [model_names[i], Accr[count,0]]\n",
        "    count += 1\n",
        "  else:\n",
        "    temp1 = [model_names[i], 0]\n",
        "  Single_and_Ensemble.loc[len(Single_and_Ensemble)] = temp1\n",
        "\n",
        "\n",
        "Single_and_Ensemble.loc[len(Single_and_Ensemble)] = ['Ensemble - Max voting' , Acc_max_voting]\n",
        "Single_and_Ensemble.loc[len(Single_and_Ensemble)] = ['Ensemble - Probability' , Acc_prob]\n",
        "\n",
        "Single_and_Ensemble.to_csv('Results/'+dataset_name+'_Ensemble.csv', index = False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Option 5 of DAFT - *DAFT-E***\n",
        "## DAFT-E : Using a few shot examples from the target domain to choose DAFT models that perform well on the target domain, and use those better performing DAFT models to perform the whole downstream task."
      ],
      "metadata": {
        "id": "_i9xCm_ookCI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at684PlwYWMw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "Regr_list = [1, 2, 4, 8, 16, 32, 64, 128]\n",
        "\n",
        "train_dataset_regr = dataset2[\"train\"].shuffle(seed=42).select([i for i in list(range(10*Regr_list[-1]))])\n",
        "y_train_Rgr = train_dataset_regr['label']\n",
        "y_train_Rgr_np = np.array(y_train_Rgr)\n",
        "\n",
        "y_pred_Rgr = np.zeros((len(train_dataset_regr),len(indx)))\n",
        "y_prob_Rgr = np.zeros((len(train_dataset_regr),len(indx)))\n",
        "\n",
        "time5 = time.time()\n",
        "for i in range(len(indx)):\n",
        "  repo_name = model_names[indx[i]]\n",
        "  pipe = pipeline(model=repo_name, device=0, token = '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')\n",
        "  y_pred1 = []\n",
        "  y_probability1 = []\n",
        "  p = 0\n",
        "  for out in pipe(KeyDataset(train_dataset_regr, feature_text), batch_size=1, truncation=\"only_first\"):\n",
        "    p += 1\n",
        "    if out['label'] == 'LABEL_1':\n",
        "      y_pred1.append(1)\n",
        "    elif out['label'] == 'LABEL_0':\n",
        "      y_pred1.append(0)\n",
        "    y_probability1.append(out['score'])\n",
        "\n",
        "  y_pred1_np = np.array(y_pred1)\n",
        "  Accuracy1  = np.sum(y_pred1_np==y_train_Rgr_np)/len(y_pred1)\n",
        "  print(Accuracy1)\n",
        "  y_pred_Rgr[:,i] = y_pred1\n",
        "  y_prob_Rgr[:,i] = y_probability1\n",
        "  torch.cuda.empty_cache()\n",
        "  # del model\n",
        "time6 = time.time()\n",
        "print(time6-time5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t95Nce6DiCk6"
      },
      "outputs": [],
      "source": [
        "# y_train_Rgr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrsz0tYobwdq"
      },
      "outputs": [],
      "source": [
        "Out_probs_Rgr = np.zeros((len(train_dataset_regr),len(indx)))\n",
        "for i in range(len(train_dataset_regr)):\n",
        "  for j in range(len(indx)):\n",
        "    if y_pred_Rgr[i,j] == 1:\n",
        "      Out_probs_Rgr[i,j] = 0.5 + (y_prob_Rgr[i,j])/2\n",
        "    elif y_pred_Rgr[i,j] == 0:\n",
        "      Out_probs_Rgr[i,j] = 0.5 - (y_prob_Rgr[i,j])/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBwhwI2Nxj1U"
      },
      "outputs": [],
      "source": [
        "from operator import itruediv\n",
        "# Linear Regression\n",
        "import random\n",
        "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "time3 = time.time()\n",
        "\n",
        "N_avg = 10\n",
        "Avg_out = np.zeros((len(Regr_list), N_avg))\n",
        "Coeff = np.zeros((len(indx), N_avg))\n",
        "\n",
        "for rgr in range(len(Regr_list)):\n",
        "  for itr in range(N_avg):\n",
        "    temp1 = y_train_Rgr_np\n",
        "    indx_K = []\n",
        "    indx_K.append(list(np.where(temp1 == 0)[0]))\n",
        "    indx_K.append(list(np.where(temp1 == 1)[0]))\n",
        "    indices = []\n",
        "    for k in range(K):\n",
        "      indices += random.sample(indx_K[k], Regr_list[rgr])\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    X_reg = Out_probs_Rgr[indices]\n",
        "    Y_reg = y_train_Rgr_np[indices]\n",
        "\n",
        "    # regr_LR = LinearRegression()\n",
        "    regr_LR = SGDRegressor(max_iter=5, random_state=rgr, alpha = 0.0001) #Actually SGD is one kind of LR # may try partial_fit instead of fit as well\n",
        "\n",
        "    regr_LR.fit(X_reg, Y_reg, coef_init=(1/X_reg.shape[1])*np.ones((X_reg.shape[1])))\n",
        "\n",
        "    print(regr_LR.coef_)\n",
        "    print(regr_LR.intercept_)\n",
        "    X_final = Out_probs\n",
        "    Y_final = regr_LR.predict(X_final)\n",
        "\n",
        "    for i in range(len(Y_final)):\n",
        "      if Y_final[i] > 0.5:\n",
        "        Y_final[i] = 1\n",
        "      elif Y_final[i] < 0.5:\n",
        "        Y_final[i] = 0\n",
        "    Y_truth = y_test_np\n",
        "\n",
        "    Acc = np.sum(Y_final == Y_truth)/len(Y_truth)\n",
        "    # print('Accuracy: '+ str(Acc))\n",
        "\n",
        "    # X_reg2 = Out_probs[0:regr_amount,:]\n",
        "    # Y_reg2 = regr_LR.predict(X_reg2)\n",
        "\n",
        "    # for i in range(len(Y_reg2)):\n",
        "    #   if Y_reg2[i] > 0.5:\n",
        "    #     Y_reg2[i] = 1\n",
        "    #   elif Y_reg2[i] < 0.5:\n",
        "    #     Y_reg2[i] = 0\n",
        "    # Y_reg2_true = y_test_np[0:regr_amount]\n",
        "\n",
        "    # Acc2 = np.sum(Y_reg2 == Y_reg2_true)/len(Y_reg2_true)\n",
        "    # print('Accuracy: '+ str(Acc2))\n",
        "\n",
        "    Avg_out[rgr, itr] = Acc\n",
        "    # Avg_out[rgr, itr, 1] = Acc2\n",
        "    Coeff[:,itr] = pd.DataFrame(regr_LR.coef_).iloc[:,0]#, columns=X_reg.columns)\n",
        "    # plt.bar(['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O'],AA.iloc[:,0])#, height, width, bottom, align)\n",
        "    # plt.show()\n",
        "time4 = time.time()\n",
        "print(time4-time3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Cq6JNUQVMe3"
      },
      "outputs": [],
      "source": [
        "Coeff_avg = np.mean(Coeff, axis=1)\n",
        "unwanted_index = [0, 6, 12]\n",
        "Model_names_concise = ['roberta-yelp', #0\n",
        "               'roberta-amazon', #1\n",
        "               'roberta-sst2',#2\n",
        "               'roberta-Tweet',#3\n",
        "               'roberta-cornel',#4\n",
        "               'roberta-imdb',#5\n",
        "               #####\n",
        "               'bert-yelp', # 6\n",
        "               'bert-amazon', # 7\n",
        "               'bert-sst2', # 8\n",
        "               'bert-Tweet', # 9\n",
        "               'bert-cornel', # 10\n",
        "               'bert-imdb', # 11\n",
        "               #####\n",
        "               'xlnet-yelp', # 12\n",
        "               'xlnet-amazon', # 13\n",
        "               'xlnet-sst2', # 14\n",
        "               'xlnet-Tweet', # 15\n",
        "               'xlnet-cornel', # 16\n",
        "               'xlnet-imdb', # 17\n",
        "               ]\n",
        "Barplot_names = [v for i,v in enumerate(Model_names_concise) if i not in unwanted_index]\n",
        "plt.bar(Barplot_names,Coeff_avg)\n",
        "plt.xticks(rotation=45, ha='right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sNxfrfy2I9P"
      },
      "outputs": [],
      "source": [
        "\n",
        "Avg_out_mean = np.mean(Avg_out, axis=1)\n",
        "Avg_out_max = np.max(Avg_out, axis=1)\n",
        "Avg_out_min = np.min(Avg_out, axis=1)\n",
        "\n",
        "\n",
        "out_LR = pd.DataFrame()\n",
        "out_LR['Train size'] = Regr_list\n",
        "out_LR['Max'] = Avg_out_max\n",
        "out_LR['Avg'] = Avg_out_mean\n",
        "out_LR['Min'] = Avg_out_min\n",
        "out_LR.to_csv('Results/'+dataset_name+'_output_LR.csv', index = False)\n",
        "# out_LR.to_csv('Results/tweet_output_LR.csv', index = False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Out_df = pd.DataFrame(Avg_out)\n",
        "# Out_df.to_csv('Results/'+dataset_name+'_Out_LR.csv', index = False)"
      ],
      "metadata": {
        "id": "3imNDC1tGWMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBqf945oopsq"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# !zip -r /content/zipped.zip /content/Results\n",
        "# files.download('/content/zipped.zip')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOQHhWwDoHHTFDqbyYt9wo6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}